\section{Обзор подходов оптимизации гиперпараметров}

\subsection{Постановка задачи}

Пусть $A$ - алгоритм, принимающий $N$ гиперпараметров для своей настройки. Будем обозначать $\Lambda_n$ - область возможных значений гиперпараметра. Пространством поиска мы назовём $\Lambda = \Lambda_1 \times \Lambda_2 \times \dots \times \Lambda_n$. Конфигурацией мы будет называть $\lambda \in \Lambda$ - вектор значений гиперпараметров, принадлежащий пространству поиска. 

Область возможных значений гиперпараметров может быть вещественнозначнной (скорость обучения), целочисленной, булевой или категориальной. 

Целевой функцией алгоритма $A$ называется вещественнозначная или целочисленная функция, принимающая на вход конфигурацию, подлежащая оптимизации. (можно сказать, что целевая функция возвращает некоторую целевую метрику, например, функция, возвращающая время компиляции некоторой программы и данное время необходимо оптимизировать), ее обозначение: $f_A$.


В данной терминологии задача ставится следующим образом: необходимо найти: 

\begin{equation}
	\lambda^* = \argmin_{\lambda \in \Lambda} f_A(\lambda)
\end{equation}

Т.е необходимо найти конфигурацию, при которой целевая функция алгоритма принимает минимальное значение. 

Стоит отметить, что пространства поиска могут быть очень большими и полный перебор всевозможных конфигураций будет невыполним. 

В данной ситуации используются итеративные методы оптимизации, их суть состоит в следующем: 

\begin{enumerate}
	\item Выбирается начальная конфигурация $\lambda_0$.
	\item Полагается $k=0$ - счётчик итераций, $I_k$ - накапливаемая информационная модель решаемой задачи.
	\item Производится вычисление значения $f_A(\lambda_k)$.
	\item Происходит пересчет информационной модели с учетом шага 3. 
	\item Анализ $I_k$ и формирование конфигурации $\lambda_{k+1}$.
	\item Проверка критерия остановки. Если критерий остановки выполняется - генерируем ответ $\lambda^*$, иначе полагаем $k:=k+1$ и переходим на шаг 3.
\end{enumerate}

\clearpage